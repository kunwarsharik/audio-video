{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audio-video_trial1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QAXWUOipag1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE-5PBvqqtR0",
        "outputId": "03c6a7b8-fcff-4132-d4cd-81eabf1bfedf"
      },
      "source": [
        "!unzip './drive/My Drive/audio-video.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./drive/My Drive/audio-video.zip\n",
            "   creating: audio-video/\n",
            " extracting: audio-video/Video_Speech_Actor_01.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_02.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_03.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_04.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_05.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_06.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_07.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_08.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_09.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_10.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_11.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_12.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_13.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_14.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_15.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_16.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_17.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_18.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_19.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_20.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_21.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_22.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_23.zip  \n",
            " extracting: audio-video/Video_Speech_Actor_24.zip  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f8l-Ev9zX_o"
      },
      "source": [
        "#!unzip 'audio-video/Video_Speech_Actor_01.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI0wevoA3kGR"
      },
      "source": [
        "import os, shutil, numpy as np, cv2,zipfile\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "from IPython.display import Audio\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten\n",
        "from tensorflow.keras.layers import Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "import tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWc4kwfuD-_z"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AXKuADoFDuJ"
      },
      "source": [
        "<h2>Loading dataset</h2>\n",
        "I start with loading dataset. Dataset contain auido-video (label=1), only video (label=2), only audio (label=3). </br>\n",
        "<b>The target of classification:</b></br>\n",
        "We have 8 labels.</br>\n",
        "01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised.</br>\n",
        "\n",
        "<b>Steps:</b> I extract the dataset initially in seprate arrays based on modality. </br>\n",
        "<b>Process of extracting video frames:</b>\n",
        "I used open CV for this purpose. Read the video frame by frame, and extract feature for the frame using pretrained resnet-50 model. We store these features.</br>\n",
        "<b>Process of extracting audio:</b>\n",
        "I used librosa library for this purpose. <b>MFCC</b> (Mel Freequency capstral coefficients). These are already proven to perform better on classification tasks.</b>\n",
        "Complete audio which is of 2,3, or 4 second is considered for MFCC feature.</b>\n",
        "\n",
        "<b>If label = audio-video.</b></br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et5M9pmaaEnU"
      },
      "source": [
        "for i in range(1,9):\n",
        "    os.mkdir(str(i))\n",
        "    #shutil.rmtree(str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDEjkIGB2Zo2"
      },
      "source": [
        "#shutil.rmtree('zipfiles')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0WMl6611J44",
        "outputId": "46beb3b4-f419-4867-e765-60beb68fe893"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m1\u001b[0m/  \u001b[01;34m2\u001b[0m/  \u001b[01;34m3\u001b[0m/  \u001b[01;34m4\u001b[0m/  \u001b[01;34m5\u001b[0m/  \u001b[01;34m6\u001b[0m/  \u001b[01;34m7\u001b[0m/  \u001b[01;34m8\u001b[0m/  \u001b[01;34maudio-video\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mzipfiles\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rSMqIzIXt4Z"
      },
      "source": [
        "arr1,arr2,arr3=[],[],[]\n",
        "label1,label2,label3=[],[],[]\n",
        "\n",
        "for act in range(1,21):\n",
        "    #'audio-video/Video_Speech_'+'Actor_'+str(act).zfill(2)+\".zip\"\n",
        "    \n",
        "    with zipfile.ZipFile('audio-video/Video_Speech_'+'Actor_'+str(act).zfill(2)+\".zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall('zipfiles')\n",
        "    \n",
        "    l=os.listdir('zipfiles/Actor_'+str(act).zfill(2)+'/')\n",
        "    for ff in range(len(l)):\n",
        "        #print('Actor_'+str(act).zfill(2)+'/'+l[ff])\n",
        "        \n",
        "        doc_name=l[ff]\n",
        "        n=doc_name.split('.mp4')\n",
        "        w=int(n[0].split('-')[0])#01 = full-AV, 02 = video-only, 03 = audio-only\n",
        "        lab=int(n[0].split('-')[2])#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
        "\n",
        "\n",
        "        if (w==1):#audio video both\n",
        "            # nn=str(vid[0])\n",
        "            # vid_name=\"/normal_final/\"+nn\n",
        "            arr=[]\n",
        "            video = cv2.VideoCapture('zipfiles/Actor_'+str(act).zfill(2)+'/'+l[ff])\n",
        "            while(True): \n",
        "                ret, frame = video.read() \n",
        "                if ret == True:  \n",
        "                    #input=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "                    frame=cv2.resize(frame,(224,224))\n",
        "                    feat=frame.reshape(1,224,224,3)\n",
        "                    arr.append(base_model.predict(feat))\n",
        "                else:\n",
        "                    print(\"done\",'zipfiles/Actor_'+str(act).zfill(2)+'/'+l[ff])\n",
        "\n",
        "                    video.release()\n",
        "                    cv2.destroyAllWindows()\n",
        "                    break\n",
        "\n",
        "            subprocess.call(['ffmpeg', '-i', 'zipfiles/Actor_'+str(act).zfill(2)+'/'+l[ff], str(lab)+'/'+'Actor_'+str(act).zfill(2)+'_'+n[0]+'.mp3'])\n",
        "            x,sr=librosa.load('1.mp3')\n",
        "            x2=librosa.to_mono(x)\n",
        "            mf=librosa.feature.mfcc(x2, sr=22050, n_mfcc=44)\n",
        "            os.remove('1.mp3') #remove a file\n",
        "            arr1.append([arr,mf])\n",
        "            label1.append(lab)\n",
        "        elif (w==2):\n",
        "            only video\n",
        "            arr=[]\n",
        "            video = cv2.VideoCapture('zipfiles/Actor_'+str(act).zfill(2)+'/'+l[ff])\n",
        "            while(True): \n",
        "                ret, frame = video.read() \n",
        "                if ret == True:  \n",
        "                    #input=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "                    frame=cv2.resize(frame,(224,224))\n",
        "                    feat=frame.reshape(1,224,224,3)\n",
        "                    arr.append(base_model.predict(feat))\n",
        "                else:\n",
        "                    print(\"done\",'zipfiles/Actor_'+str(act).zfill(2)+'/'+l[ff])\n",
        "                    video.release()\n",
        "                    cv2.destroyAllWindows()\n",
        "                    break\n",
        "            arr2.append(arr)\n",
        "            label2.append(lab)\n",
        "        else:\n",
        "            only audio\n",
        "            mf=0\n",
        "            subprocess.call(['ffmpeg', '-i', 'zipfiles/Actor_'+str(act).zfill(2)+'/'+l[ff], '1.mp3'])\n",
        "            x,sr=librosa.load('1.mp3')\n",
        "            x2=librosa.to_mono(x)\n",
        "            mf=librosa.feature.mfcc(x2, sr=22050, n_mfcc=44)\n",
        "            os.remove('1.mp3') #remove a file\n",
        "            arr3.append(mf)\n",
        "            label3.append(lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxswSwoIyfFU",
        "outputId": "6fc234a0-9994-4155-b110-5c29ad17ce8b"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m1\u001b[0m/  \u001b[01;34m2\u001b[0m/  \u001b[01;34m3\u001b[0m/  \u001b[01;34m4\u001b[0m/  \u001b[01;34m5\u001b[0m/  \u001b[01;34m6\u001b[0m/  \u001b[01;34m7\u001b[0m/  \u001b[01;34m8\u001b[0m/  \u001b[01;34maudio-video\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mzipfiles\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9UHJd6NziQE"
      },
      "source": [
        "n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cr8ku9FyP3A"
      },
      "source": [
        "for i in range(1,9):\n",
        "    shutil.copytree(str(i),'./drive/My Drive/'+str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al6YRK-WNXr4"
      },
      "source": [
        "def feature6(audio):\n",
        "  #librosa.feature.chroma_stft(audio,sr=22050),librosa.feature.chroma_cqt(audio,sr=22050), \n",
        "  return np.concatenate((librosa.feature.chroma_cens(audio,sr=22050),librosa.feature.mfcc(audio,sr=22050,n_mfcc=44) ,librosa.feature.melspectrogram(y=audio,sr=22050),\n",
        "               librosa.feature.rms(audio), #librosa.feature.rmse(audio), \n",
        "               librosa.feature.spectral_centroid(audio,sr=22050),librosa.feature.tempogram(audio,sr=22050),\n",
        "               librosa.feature.spectral_bandwidth(audio,sr=22050),librosa.feature.spectral_contrast(audio,sr=22050),librosa.feature.spectral_flatness(audio),librosa.feature.spectral_rolloff(audio,sr=22050),\n",
        "               librosa.feature.poly_features(audio,sr=22050),librosa.feature.tonnetz(audio,sr=22050),librosa.feature.zero_crossing_rate(audio)),axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7cFu1Q057A2"
      },
      "source": [
        "arr1,lab=[],[]\n",
        "for i in range(1,9):\n",
        "    #arr=[]\n",
        "    for j in os.listdir('./drive/My Drive/'+str(i)):\n",
        "        x,sr=librosa.load('./drive/My Drive/'+str(i)+'/'+j)\n",
        "        arr1.append(feature6(x))\n",
        "        lab.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE6ZjtTY_A28"
      },
      "source": [
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKCHhAA4-vxa"
      },
      "source": [
        "# arr,label = shuffle(arr1, lab, random_state=0)\n",
        "# arr1=0\n",
        "# lab=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvt9j76KGmXl",
        "outputId": "61361d1d-f1e4-4f65-8926-68723eebf60b"
      },
      "source": [
        "arrmax, arrmin=0,10000\n",
        "for i in arr:#[0].shape\n",
        "    if arrmax<i.shape[1]:\n",
        "        arrmax=i.shape[1]\n",
        "    if arrmin>i.shape[1]:\n",
        "        arrmin=i.shape[1]\n",
        "print(arrmin,arrmax)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "127 228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgSea8F5I8DU",
        "outputId": "e4173463-f167-4d31-e70e-f7cf4ee5a7eb"
      },
      "source": [
        "arr2=[]\n",
        "for i in arr:\n",
        "    diff=arrmax-i.shape[1]\n",
        "    arr2.append(np.concatenate((i,np.zeros((i.shape[0],diff))),axis=1))\n",
        "arr2=np.array(arr2)\n",
        "arr2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 589, 228)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyY0xfDBE9Sg"
      },
      "source": [
        "arr2=np.reshape(arr2,(len(arr2),-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkN-zBPp_FlS",
        "outputId": "2e538c6a-3105-44e7-c15b-7bddff33de52"
      },
      "source": [
        "#arr2=np.array(arr)\n",
        "# label=np.array(label)\n",
        "# label=label-1\n",
        "# label=to_categorical(label)\n",
        "arr2.shape,label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1200, 134292), (1200, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIiKNyW_LIdB",
        "outputId": "b8213ef5-685f-4369-bb13-c220bbb6526b"
      },
      "source": [
        "min(arr2.ravel()), max(arr2.ravel())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1093.0296630859375, 10895.80078125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riPzkwj5Lbhv"
      },
      "source": [
        "arr2= (arr2 - )/()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCSwEjLHAp8i"
      },
      "source": [
        "input = Input(shape=(134292), name='image_input')\n",
        "#x = Flatten()(base_model(input))\n",
        "x = Dense(300, activation='tanh')(input)\n",
        "#x = Dense(100, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(8, activation='softmax')(x)\n",
        "model = Model(inputs=input, outputs=x)\n",
        "opt = SGD(learning_rate=1e-3)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(arr2,label, validation_split=0.15,epochs=50)#, batch_size=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2wwCLZFPOPv"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPWOEwniPRKI"
      },
      "source": [
        "from tensorflow.keras import Sequential, layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CENpVui-Ov7A"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Conv1D(256, 5,padding='same',input_shape=(236,40)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling1D(pool_size=(8)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "model.add(layers.Conv1D(128, 5,padding='same'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling1D(pool_size=(4)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64))\n",
        "model.add(layers.Dense(8))\n",
        "model.add(layers.Activation('softmax'))\n",
        "\n",
        "opt = SGD(learning_rate=1e-3)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(arr2,label, validation_split=0.15,epochs=50)#, batch_size=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ-S9p_THVg_",
        "outputId": "c210ea6f-d3b3-40e8-8a8f-18ce20e2972a"
      },
      "source": [
        "# np.save('./drive/My Drive/audio_video/11',arr1)\n",
        "# np.save('./drive/My Drive/audio_video/12',arr2)\n",
        "# np.save('./drive/My Drive/audio_video/13',arr3)\n",
        "# np.save('./drive/My Drive/audio_video/11l',label1)\n",
        "# np.save('./drive/My Drive/audio_video/12l',label2)\n",
        "# np.save('./drive/My Drive/audio_video/13l',label3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gXSma__HF9i"
      },
      "source": [
        "# files.download('11.npy')\n",
        "# files.download('12.npy')\n",
        "# files.download('13.npy')\n",
        "# files.download('11l.npy')\n",
        "# files.download('12l.npy')\n",
        "# files.download('13l.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7DwHtagEPNV",
        "outputId": "ce10d9d2-59d8-4ee1-e33c-3d846b6d2f2b"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive/\")\n",
        "# from google.colab import drive\n",
        "# drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYoYQazUHzJx"
      },
      "source": [
        "Colab provide only 2 cores. 1 run code other run interface. So mutithreading in one file is not efficient.<br>\n",
        "Hence, I choose to run same code in multiple colab accounts. Actor 1-8 run in this file, Actor 8-15 in second and Actor 15-21 in third. There is no chage in code; Just while loding dataset step, range (1,8) is replaced. However they are stored in one drive, to avoid running again. Also colab GPU is available for limited time.</br>\n",
        "Below three code blocks load load the feature and label arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zai9YTchKLFi",
        "outputId": "995d260b-32ce-44f7-8209-b58b520e4eab"
      },
      "source": [
        "len(arr1),len(label1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 1200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkxiyZXxKw_U",
        "outputId": "54d4675b-d36d-43cc-db24-63496d9b5601"
      },
      "source": [
        "arr1= np.array(arr1)\n",
        "label1= np.array(label1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYImWqxmLB4R"
      },
      "source": [
        "arr1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COuXaojy8cRn"
      },
      "source": [
        "audio=[]\n",
        "for i in arr1:\n",
        "    audio.append(i[1])\n",
        "#audio=arr1\n",
        "label=label1\n",
        "audio_img=[]\n",
        "for i in audio:\n",
        "    fig = plt.Figure(frameon=False)\n",
        "    fig.set_size_inches(2,1)\n",
        "    canvas = FigureCanvas(fig)\n",
        "    #ax = fig.add_subplot(111)\n",
        "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
        "    ax.set_axis_off()\n",
        "    fig.add_axes(ax)\n",
        "    #mfccs = librosa.feature.mfcc(audio[0], sr=22050)\n",
        "    ax.imshow(i, aspect='auto')\n",
        "    fig.savefig('spec1.png')\n",
        "    audio_img.append(cv2.resize(cv2.imread('spec1.png'),(224,224)))\n",
        "    os.remove('spec1.png')\n",
        "    #plt.imshow(plt.imread('spec1.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV-UptKg-FgE",
        "outputId": "cdc04f35-9614-4164-fa11-cb266a8575b6"
      },
      "source": [
        "audio_img=np.array(audio_img)\n",
        "label= np.array(label)\n",
        "label=label-1\n",
        "label=to_categorical(label)\n",
        "audio_img.shape,label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1200, 224, 224, 3), (1200, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdsCFjcVMCHw"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=True)#, input_tensor=Input(shape=(237, 237, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvOtiq4aMUyd"
      },
      "source": [
        "# audio_img=audio_img/255.0\n",
        "audio_img[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P5aJip6SC8I"
      },
      "source": [
        "Accuracy using Just from audio "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFB70jws-rqC",
        "outputId": "1b28eabc-e50e-4572-aa53-46b7eb653f24"
      },
      "source": [
        "input = Input(shape=(224,224, 3), name='image_input')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "x = Flatten()(base_model(input))\n",
        "x = Dense(200, activation='relu')(x)\n",
        "#x = Dense(100, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(8, activation='softmax')(x)\n",
        "model = Model(inputs=input, outputs=x)\n",
        "opt = SGD(learning_rate=1e-4)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(audio_img,label, validation_split=0.15,epochs=50)#, batch_size=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "32/32 [==============================] - 15s 346ms/step - loss: 2.0797 - accuracy: 0.0980 - val_loss: 2.0789 - val_accuracy: 0.1667\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 10s 311ms/step - loss: 2.0795 - accuracy: 0.1020 - val_loss: 2.0796 - val_accuracy: 0.1167\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 10s 312ms/step - loss: 2.0796 - accuracy: 0.0931 - val_loss: 2.0795 - val_accuracy: 0.1278\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 10s 313ms/step - loss: 2.0794 - accuracy: 0.1049 - val_loss: 2.0793 - val_accuracy: 0.0778\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 10s 313ms/step - loss: 2.0795 - accuracy: 0.1275 - val_loss: 2.0792 - val_accuracy: 0.1222\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 10s 314ms/step - loss: 2.0794 - accuracy: 0.1225 - val_loss: 2.0802 - val_accuracy: 0.1500\n",
            "Epoch 7/50\n",
            "32/32 [==============================] - 10s 316ms/step - loss: 2.0795 - accuracy: 0.1275 - val_loss: 2.0795 - val_accuracy: 0.1444\n",
            "Epoch 8/50\n",
            "32/32 [==============================] - 10s 316ms/step - loss: 2.0793 - accuracy: 0.1216 - val_loss: 2.0793 - val_accuracy: 0.1222\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - 10s 319ms/step - loss: 2.0794 - accuracy: 0.1176 - val_loss: 2.0790 - val_accuracy: 0.1333\n",
            "Epoch 10/50\n",
            "32/32 [==============================] - 10s 319ms/step - loss: 2.0793 - accuracy: 0.1225 - val_loss: 2.0793 - val_accuracy: 0.1056\n",
            "Epoch 11/50\n",
            "32/32 [==============================] - 10s 320ms/step - loss: 2.0795 - accuracy: 0.1137 - val_loss: 2.0792 - val_accuracy: 0.1556\n",
            "Epoch 12/50\n",
            "32/32 [==============================] - 10s 320ms/step - loss: 2.0793 - accuracy: 0.1157 - val_loss: 2.0788 - val_accuracy: 0.1167\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - 10s 322ms/step - loss: 2.0793 - accuracy: 0.1216 - val_loss: 2.0785 - val_accuracy: 0.0889\n",
            "Epoch 14/50\n",
            "32/32 [==============================] - 10s 323ms/step - loss: 2.0793 - accuracy: 0.1265 - val_loss: 2.0785 - val_accuracy: 0.1056\n",
            "Epoch 15/50\n",
            "32/32 [==============================] - 10s 323ms/step - loss: 2.0792 - accuracy: 0.1284 - val_loss: 2.0784 - val_accuracy: 0.1167\n",
            "Epoch 16/50\n",
            "32/32 [==============================] - 10s 324ms/step - loss: 2.0793 - accuracy: 0.1176 - val_loss: 2.0784 - val_accuracy: 0.0944\n",
            "Epoch 17/50\n",
            "32/32 [==============================] - 10s 325ms/step - loss: 2.0791 - accuracy: 0.1333 - val_loss: 2.0785 - val_accuracy: 0.1056\n",
            "Epoch 18/50\n",
            "32/32 [==============================] - 10s 325ms/step - loss: 2.0792 - accuracy: 0.1245 - val_loss: 2.0785 - val_accuracy: 0.0889\n",
            "Epoch 19/50\n",
            "32/32 [==============================] - 10s 326ms/step - loss: 2.0792 - accuracy: 0.1235 - val_loss: 2.0785 - val_accuracy: 0.0944\n",
            "Epoch 20/50\n",
            "32/32 [==============================] - 10s 326ms/step - loss: 2.0791 - accuracy: 0.1235 - val_loss: 2.0785 - val_accuracy: 0.0944\n",
            "Epoch 21/50\n",
            "32/32 [==============================] - 10s 327ms/step - loss: 2.0792 - accuracy: 0.1098 - val_loss: 2.0785 - val_accuracy: 0.0778\n",
            "Epoch 22/50\n",
            "32/32 [==============================] - 10s 328ms/step - loss: 2.0789 - accuracy: 0.1294 - val_loss: 2.0785 - val_accuracy: 0.0833\n",
            "Epoch 23/50\n",
            "32/32 [==============================] - 10s 328ms/step - loss: 2.0790 - accuracy: 0.1265 - val_loss: 2.0784 - val_accuracy: 0.0889\n",
            "Epoch 24/50\n",
            "32/32 [==============================] - 11s 329ms/step - loss: 2.0788 - accuracy: 0.1392 - val_loss: 2.0784 - val_accuracy: 0.0778\n",
            "Epoch 25/50\n",
            "32/32 [==============================] - 11s 330ms/step - loss: 2.0789 - accuracy: 0.1314 - val_loss: 2.0784 - val_accuracy: 0.0944\n",
            "Epoch 26/50\n",
            "32/32 [==============================] - 11s 330ms/step - loss: 2.0790 - accuracy: 0.1304 - val_loss: 2.0783 - val_accuracy: 0.0944\n",
            "Epoch 27/50\n",
            "32/32 [==============================] - 11s 330ms/step - loss: 2.0790 - accuracy: 0.1314 - val_loss: 2.0783 - val_accuracy: 0.1000\n",
            "Epoch 28/50\n",
            "32/32 [==============================] - 11s 330ms/step - loss: 2.0789 - accuracy: 0.1275 - val_loss: 2.0783 - val_accuracy: 0.1000\n",
            "Epoch 29/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0787 - accuracy: 0.1392 - val_loss: 2.0783 - val_accuracy: 0.1056\n",
            "Epoch 30/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0788 - accuracy: 0.1324 - val_loss: 2.0782 - val_accuracy: 0.1111\n",
            "Epoch 31/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0787 - accuracy: 0.1176 - val_loss: 2.0782 - val_accuracy: 0.1167\n",
            "Epoch 32/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0787 - accuracy: 0.1333 - val_loss: 2.0782 - val_accuracy: 0.1167\n",
            "Epoch 33/50\n",
            "32/32 [==============================] - 11s 332ms/step - loss: 2.0788 - accuracy: 0.1255 - val_loss: 2.0782 - val_accuracy: 0.1167\n",
            "Epoch 34/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0790 - accuracy: 0.1304 - val_loss: 2.0781 - val_accuracy: 0.1222\n",
            "Epoch 35/50\n",
            "32/32 [==============================] - 11s 332ms/step - loss: 2.0787 - accuracy: 0.1373 - val_loss: 2.0781 - val_accuracy: 0.1222\n",
            "Epoch 36/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0784 - accuracy: 0.1343 - val_loss: 2.0781 - val_accuracy: 0.1278\n",
            "Epoch 37/50\n",
            "32/32 [==============================] - 11s 332ms/step - loss: 2.0786 - accuracy: 0.1147 - val_loss: 2.0781 - val_accuracy: 0.1222\n",
            "Epoch 38/50\n",
            "32/32 [==============================] - 11s 332ms/step - loss: 2.0786 - accuracy: 0.1186 - val_loss: 2.0780 - val_accuracy: 0.1222\n",
            "Epoch 39/50\n",
            "32/32 [==============================] - 11s 332ms/step - loss: 2.0785 - accuracy: 0.1176 - val_loss: 2.0781 - val_accuracy: 0.1333\n",
            "Epoch 40/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0788 - accuracy: 0.1324 - val_loss: 2.0780 - val_accuracy: 0.1333\n",
            "Epoch 41/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0787 - accuracy: 0.1157 - val_loss: 2.0780 - val_accuracy: 0.1278\n",
            "Epoch 42/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0784 - accuracy: 0.1529 - val_loss: 2.0779 - val_accuracy: 0.1333\n",
            "Epoch 43/50\n",
            "32/32 [==============================] - 11s 332ms/step - loss: 2.0785 - accuracy: 0.1176 - val_loss: 2.0778 - val_accuracy: 0.1222\n",
            "Epoch 44/50\n",
            "32/32 [==============================] - 11s 332ms/step - loss: 2.0785 - accuracy: 0.1275 - val_loss: 2.0778 - val_accuracy: 0.1222\n",
            "Epoch 45/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0785 - accuracy: 0.1235 - val_loss: 2.0777 - val_accuracy: 0.1222\n",
            "Epoch 46/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0781 - accuracy: 0.1412 - val_loss: 2.0776 - val_accuracy: 0.1056\n",
            "Epoch 47/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0785 - accuracy: 0.1294 - val_loss: 2.0777 - val_accuracy: 0.1111\n",
            "Epoch 48/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0785 - accuracy: 0.1353 - val_loss: 2.0777 - val_accuracy: 0.1167\n",
            "Epoch 49/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0783 - accuracy: 0.1157 - val_loss: 2.0776 - val_accuracy: 0.1111\n",
            "Epoch 50/50\n",
            "32/32 [==============================] - 11s 331ms/step - loss: 2.0781 - accuracy: 0.1284 - val_loss: 2.0776 - val_accuracy: 0.1111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f05a2b49b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1ZWVmWD30g9"
      },
      "source": [
        "arr1= np.load('./drive/My Drive/audio_video/11.npy', allow_pickle=True)\n",
        "arr2= np.load('./drive/My Drive/audio_video/12.npy', allow_pickle=True)\n",
        "arr3= np.load('./drive/My Drive/audio_video/13.npy',allow_pickle=True)\n",
        "label1=np.load('./drive/My Drive/audio_video/11l.npy',allow_pickle=True)\n",
        "label2=np.load('./drive/My Drive/audio_video/12l.npy',allow_pickle=True)\n",
        "label3=np.load('./drive/My Drive/audio_video/13l.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ZqHLQKAKKH"
      },
      "source": [
        "a21=np.load('./drive/My Drive/audio_video/21.npy',allow_pickle=True)\n",
        "a22=np.load('./drive/My Drive/audio_video/22.npy',allow_pickle=True)\n",
        "a23=np.load('./drive/My Drive/audio_video/23.npy',allow_pickle=True)\n",
        "a21l=np.load('./drive/My Drive/audio_video/21l.npy',allow_pickle=True)\n",
        "a22l=np.load('./drive/My Drive/audio_video/22l.npy',allow_pickle=True)\n",
        "a23l=np.load('./drive/My Drive/audio_video/23l.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0NTshhJAuFk"
      },
      "source": [
        "a31=np.load('./drive/My Drive/audio_video/31.npy',allow_pickle=True)\n",
        "a32=np.load('./drive/My Drive/audio_video/32.npy',allow_pickle=True)\n",
        "a33=np.load('./drive/My Drive/audio_video/33.npy',allow_pickle=True)\n",
        "a31l=np.load('./drive/My Drive/audio_video/31l.npy',allow_pickle=True)\n",
        "a32l=np.load('./drive/My Drive/audio_video/32l.npy',allow_pickle=True)\n",
        "a33l=np.load('./drive/My Drive/audio_video/33l.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_zgQT5cJGGk"
      },
      "source": [
        "MFCC feature will represent audio of video. Video feature are already normalized from Resnet model. In this step, To Normalize the MFCC feature , We find min and max of feature value in training set i.e. (Actor 1 to 21). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spOsKGpoDt5S"
      },
      "source": [
        "amin=10000\n",
        "amax=0\n",
        "for i in arr1:\n",
        "    dmin=min(np.ravel(i[1]))\n",
        "    dmax=max(np.ravel(i[1]))\n",
        "    if amin>dmin:\n",
        "        amin=dmin\n",
        "    if amax<dmax:\n",
        "        amax=dmax\n",
        "\n",
        "for i in a21:\n",
        "    dmin=min(np.ravel(i[1]))\n",
        "    dmax=max(np.ravel(i[1]))\n",
        "    if amin>dmin:\n",
        "        amin=dmin\n",
        "    if amax<dmax:\n",
        "        amax=dmax\n",
        "\n",
        "for i in a31:\n",
        "    dmin=min(np.ravel(i[1]))\n",
        "    dmax=max(np.ravel(i[1]))\n",
        "    if amin>dmin:\n",
        "        amin=dmin\n",
        "    if amax<dmax:\n",
        "        amax=dmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UZywoDGExpE",
        "outputId": "de0446d0-365b-4f2b-9769-55ae242eaa91"
      },
      "source": [
        "amax,amin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(237.81725, -1093.0297)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJLOP9ATNdBR"
      },
      "source": [
        "<h3>Combining the arrays.</h3> </br></br>\n",
        "a1 is array where each element contain video(frame by frame resnet features) and audio(MFCC) features. </br></br>\n",
        "a2's each element is just resnet featues as this contains data samples with only video.</br></br>\n",
        "a3's each element is audio only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr31fKZqA9v0"
      },
      "source": [
        "a1=np.concatenate((arr1,a21,a31),axis=0)\n",
        "a2=np.concatenate((arr2,a22,a32),axis=0)\n",
        "a3=np.concatenate((arr3,a23,a33),axis=0)\n",
        "\n",
        "a1l=np.concatenate((label1,a21l,a31l),axis=0)\n",
        "a2l=np.concatenate((label2,a22l,a32l),axis=0)\n",
        "a3l=np.concatenate((label3,a23l,a33l),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4_VXZ3g_ZIi",
        "outputId": "7c6d9661-06f4-492f-a6d9-35bc40117d85"
      },
      "source": [
        "a1.shape,a1l.shape, a2.shape, a2l.shape, a3.shape, a3l.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1200, 2), (2400,), (1200,), (0,), (0,), (0,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N48WyhJuE_mV"
      },
      "source": [
        "# ar=(arr1[0][1]-amin)/(amax-amin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U84O0XuqDlPn",
        "outputId": "e126f37e-2112-4a4e-eb81-57fc40e6e2c4"
      },
      "source": [
        "np.array(a2[0]).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97, 1, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmR1ndwjOUga"
      },
      "source": [
        "To use Deep Learning or any ML model wE need fixed size input.</br> We check feature shapes and get min and max feature shapes.</br>\n",
        "Below different feature shapes are recorded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NinwO37rTKZ6"
      },
      "source": [
        "#different shapes of video feature\n",
        "vfs=[]#vid_feat_shape\n",
        "for vf in a1:\n",
        "    if np.array(vf[0]).shape not in vfs:\n",
        "        vfs.append(np.array(vf[0]).shape)\n",
        "for vf in a2:\n",
        "    if np.array(vf).shape not in vfs:\n",
        "        vfs.append(np.array(vf).shape)\n",
        "\n",
        "mfs=[]#mfcc_feat_shape\n",
        "for mf in a1:\n",
        "    if np.array(mf[1]).shape not in mfs:\n",
        "        mfs.append(np.array(mf[1]).shape)\n",
        "for mf in a3:\n",
        "    if mf.shape not in mfs:\n",
        "        mfs.append(np.array(mf).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awjr35wrO0c8"
      },
      "source": [
        "min and max feature shapes for video and audio are obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8x6LWTGWEiF"
      },
      "source": [
        "imax=0\n",
        "imin=10000\n",
        "for i in vfs:\n",
        "    if i[0]>imax:\n",
        "        imax=i[0]\n",
        "    if i[0]<imin:\n",
        "        imin=i[0]\n",
        "jmax,jmin=0,10000\n",
        "for i in mfs:\n",
        "    if i[1]>jmax:\n",
        "        jmax=i[1]\n",
        "    if i[1]<jmin:\n",
        "        jmin=i[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a1UD-sAGsp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd9f017-4d0c-4f84-cedf-bce1bb34e56b"
      },
      "source": [
        "imin,imax,jmin,jmax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88, 157, 127, 228)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBBPLA4iO7cJ"
      },
      "source": [
        "Making all feature video and audio respectively to same size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHprTJ4p-LfV"
      },
      "source": [
        "#np.array(a1[0][0]).reshape(len(a1[0][0]))\n",
        "#def video_frame_bal(a1)\n",
        "a1_1=[]\n",
        "for i in a1:\n",
        "    arr=np.array(i[0])\n",
        "    arr=arr.reshape(arr.shape[0],arr.shape[2])\n",
        "    diff=imax-arr.shape[0]\n",
        "    if diff%2==1:\n",
        "        front=np.zeros((diff//2,1000),dtype='float')\n",
        "        end=np.zeros(((diff//2)+1,1000),dtype='float')\n",
        "    else:\n",
        "        front=np.zeros((diff//2,1000),dtype='float')\n",
        "        end=np.zeros((diff//2,1000),dtype='float')\n",
        "    a1_1.append(np.concatenate((front,arr,end), axis=0))\n",
        "a1_1=np.array(a1_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gSMs0zZsRO0"
      },
      "source": [
        "#def audio_frame_bal(a1):\n",
        "jmax=250\n",
        "a1_2=[]\n",
        "for i in a1:\n",
        "    arr=i[1]\n",
        "    diff=jmax-arr.shape[1]\n",
        "    if diff%2==1:\n",
        "        front=np.zeros((44,diff//2),dtype='float')\n",
        "        end=np.zeros((44,(diff//2)+1),dtype='float')\n",
        "    else:\n",
        "        front=np.zeros((44,diff//2),dtype='float')\n",
        "        end=np.zeros((44,diff//2),dtype='float')\n",
        "    a1_2.append(np.concatenate((front,arr,end), axis=1))\n",
        "a1_2= np.array(a1_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g91Noa-4tcRx"
      },
      "source": [
        "a2_1=[]\n",
        "for i in a2:\n",
        "    arr=np.array(i)\n",
        "    arr=arr.reshape(arr.shape[0],arr.shape[2])\n",
        "    diff=imax-arr.shape[0]\n",
        "    if diff%2==1:\n",
        "        front=np.zeros((diff//2,1000),dtype='float')\n",
        "        end=np.zeros(((diff//2)+1,1000),dtype='float')\n",
        "    else:\n",
        "        front=np.zeros((diff//2,1000),dtype='float')\n",
        "        end=np.zeros((diff//2,1000),dtype='float')\n",
        "    a2_1.append(np.concatenate((front,arr,end), axis=0))\n",
        "a2_1=np.array(a2_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZOt0Al7Ljj_"
      },
      "source": [
        "# making audio with zero array for the sample with only video. Now all samples have audio and video.\n",
        "a2_2=np.zeros((1200,44,250), dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLjaFUTMGfgy",
        "outputId": "444442d2-6b3f-4752-d0c7-4310cd112380"
      },
      "source": [
        "a1_1.shape,a1_2.shape, a2_1.shape, a2_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1200, 157, 1000), (1200, 44, 250), (1200, 157, 1000), (1200, 44, 250))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL3XLroXPKZE"
      },
      "source": [
        "Normalize audio feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXY7-AlRqkVE"
      },
      "source": [
        "a1_2= (a1_2-amin)/(amax-amin)\n",
        "a2_2= (a2_2-amin)/(amax-amin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9llOzYaPPLv"
      },
      "source": [
        "Reshape Audio feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgHpJI13HCmu"
      },
      "source": [
        "a1_2= a1_2.reshape(a1_2.shape[0],-1,1000)\n",
        "a2_2= a2_2.reshape(a2_2.shape[0],-1,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNZnd17eJWYG",
        "outputId": "dd8d51db-bef5-42a3-a788-556977ab8f4c"
      },
      "source": [
        "a1_1.shape,a1_2.shape, a2_1.shape, a2_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1200, 157, 1000), (1200, 11, 1000), (1200, 157, 1000), (1200, 11, 1000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W84cqb4KhHR"
      },
      "source": [
        "A11=np.concatenate((a1_1,a1_2), axis=1)\n",
        "A22=np.concatenate((a2_1,a2_2), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCkYuPsDOBMN",
        "outputId": "ef791f32-aab3-4197-a868-4c26002c741c"
      },
      "source": [
        "A11.shape,A22.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1200, 168, 1000), (1200, 168, 1000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6hPVwkyPpyr"
      },
      "source": [
        "Each sample now is of uniform shape. and contain audio and video feature. \n",
        "Finally we have data to train our multimodal algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1EpEJ6rOJMt"
      },
      "source": [
        "data= np.concatenate((A11,A22),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElcZUvhrSCQS"
      },
      "source": [
        "label= np.concatenate((label1,a21l,a31l,label2,a22l,a32l),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRd7JCc4RpxB",
        "outputId": "a7b8a899-4e6a-455d-cc05-c99a29648621"
      },
      "source": [
        "data.shape,label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2400, 168, 1000), (2400,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esTqQz0JQRXP"
      },
      "source": [
        "Just free_ing space ram is full."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1RJNosCSHS9"
      },
      "source": [
        "A11,A22, a1, a2,a3,a1l,a2l,a3l=0,0,0,0,0,0,0,0\n",
        "arr1,arr2,arr3,label1,label2,label3=0,0,0,0,0,0\n",
        "a21,a22,a23,a21l,a22l,a23l=0,0,0,0,0,0\n",
        "a31,a32,a33,a31l,a32l,a33l=0,0,0,0,0,0\n",
        "a1_1,a1_2, a2_1, a2_2= 0,0,0,0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaQA_ecvQl0M"
      },
      "source": [
        "To create an image type input with three channel.</br>\n",
        "168507/3= 56169</br>\n",
        "237*237 = 56169"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzd2B-01WiJT"
      },
      "source": [
        "data= data.reshape(len(data),-1)\n",
        "data=np.concatenate((data,np.zeros((len(data),507), dtype=float)),axis=1)\n",
        "data= data.reshape(len(data), 237,237,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhEBOBGCRIgU"
      },
      "source": [
        "label from numral to categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JDdqlvIXl3m"
      },
      "source": [
        "label2=[]\n",
        "for i in label:\n",
        "    label2.append(i-1)\n",
        "label= np.array(label2,dtype=int)\n",
        "label=to_categorical(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbGBIqFORSGT"
      },
      "source": [
        "(OPTIONAL)</br>\n",
        "Trying to use normal resnet nput shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXyJ_TlyfcVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f372b592-2a45-463e-aef0-47bad3310efe"
      },
      "source": [
        "data2=[]\n",
        "for i in data:\n",
        "    data2.append(cv2.resize(i,(224,224)))\n",
        "data2=np.array(data2)\n",
        "data2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2400, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZIH3UnuUJva",
        "outputId": "da998ce7-4f69-49ae-8e3c-a82da0b8e5d1"
      },
      "source": [
        "data.shape, label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2400, 237, 237, 3), (2400, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2pX9dU2aX1U",
        "outputId": "c549b2f7-f92e-4cb6-f1b0-8540d37bcacd"
      },
      "source": [
        "# base_model.layers.pop(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f5897df5e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_JIxZaTRgKU"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvvTvXhhEB6M"
      },
      "source": [
        "input = Input(shape=(224,224, 3), name='image_input')\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "x = Flatten()(base_model(input))\n",
        "x = Dense(200, activation='relu')(x)\n",
        "#x = Dense(100, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(8, activation='softmax')(x)\n",
        "model = Model(inputs=input, outputs=x)\n",
        "opt = SGD(learning_rate=1e-4)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(data2,label, validation_split=0.15,epochs=50)#, batch_size=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvTFOu7FRk4l"
      },
      "source": [
        "The dataset till actor 20 don't have only audio samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEjs0HdDGkoE"
      },
      "source": [
        "#def audio_frame_bal(a1):\n",
        "a3_1=[]\n",
        "for i in a3:\n",
        "    arr=i\n",
        "    diff=jmax-arr.shape[1]\n",
        "    if diff%2==1:\n",
        "        front=np.zeros((44,diff//2),dtype='float')\n",
        "        end=np.zeros((44,(diff//2)+1),dtype='float')\n",
        "    else:\n",
        "        front=np.zeros((44,diff//2),dtype='float')\n",
        "        end=np.zeros((44,diff//2),dtype='float')\n",
        "    a3_1.append(np.concatenate((front,arr,end), axis=1))\n",
        "a3_1= np.array(a3_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArOLHLb-G7Qq",
        "outputId": "16bcce82-6b46-4c13-b1fb-948861ba09a0"
      },
      "source": [
        "a3_1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue-E2j7d8RZe"
      },
      "source": [
        "# #files.download('1.mp3')\n",
        "# Audio('1.mp3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7Wgr5fdU_Pp",
        "outputId": "b2ac7326-d3f8-436a-ed84-a5b3c555fb26"
      },
      "source": [
        "# doc_name='02-01-02-02-02-02-01.mp4'\n",
        "# n=doc_name.split('.mp4')\n",
        "# w=int(n[0].split('-')[0])#01 = full-AV, 02 = video-only, 03 = audio-only\n",
        "# l=int(n[0].split('-')[2])#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
        "# if (w==1):#audio video both\n",
        "#     # nn=str(vid[0])\n",
        "#     # vid_name=\"/normal_final/\"+nn\n",
        "#     arr=[]\n",
        "#     video = cv2.VideoCapture(doc_name)\n",
        "#     while(True): \n",
        "#         ret, frame = video.read() \n",
        "#         if ret == True:  \n",
        "#             #input=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "#             frame=cv2.resize(frame,(224,224))\n",
        "#             feat=frame.reshape(1,224,224,3)\n",
        "#             arr.append(base_model.predict(feat))\n",
        "#         else:\n",
        "#             print(\"done\",vid[0])\n",
        "#             video.release()\n",
        "#             cv2.destroyAllWindows()\n",
        "#             break\n",
        "    \n",
        "#     subprocess.call(['ffmpeg', '-i', 'Actor_01/'+l[3], '1.mp3'])\n",
        "#     x,sr=librosa.load('1.mp3')\n",
        "#     x2=librosa.to_mono(x)\n",
        "#     mf.append(librosa.feature.mfcc(x2, sr=22050, n_mfcc=44))\n",
        "        \n",
        "# elif (w==2):\n",
        "#     #only video\n",
        "#     arr=[]\n",
        "#     video = cv2.VideoCapture(doc_name)\n",
        "#     while(True): \n",
        "#         ret, frame = video.read() \n",
        "#         if ret == True:  \n",
        "#             #input=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "#             frame=cv2.resize(frame,(224,224))\n",
        "#             feat=frame.reshape(1,224,224,3)\n",
        "#             arr.append(base_model.predict(feat))\n",
        "#         else:\n",
        "#             print(\"done\",vid[0])\n",
        "#             video.release()\n",
        "#             cv2.destroyAllWindows()\n",
        "#             break\n",
        "# else:\n",
        "#     #only audio\n",
        "#     subprocess.call(['ffmpeg', '-i', 'Actor_01/'+l[3], '1.mp3'])\n",
        "#     x,sr=librosa.load('1.mp3')\n",
        "#     x2=librosa.to_mono(x)\n",
        "#     mf.append(librosa.feature.mfcc(x2, sr=22050, n_mfcc=44))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRUok6SlFY2-"
      },
      "source": [
        "# files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}